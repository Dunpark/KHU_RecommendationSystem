# Phase 1-2에서 사용된 데이터 분석 개념 및 기법

**경희대학교 빅데이터응용학과 추천시스템 프로젝트**  
**작성일**: 2024.12.05

---

# 📋 목차

1. [탐색적 데이터 분석 (EDA)](#1-탐색적-데이터-분석-eda)
2. [분포 법칙 및 통계 개념](#2-분포-법칙-및-통계-개념)
3. [추천시스템 핵심 개념](#3-추천시스템-핵심-개념)
4. [데이터 분할 기법](#4-데이터-분할-기법)
5. [데이터 구조 및 변환](#5-데이터-구조-및-변환)
6. [평가 지표](#6-평가-지표)
7. [피처 엔지니어링](#7-피처-엔지니어링)

---

# 1. 탐색적 데이터 분석 (EDA)

## 1.1 기술 통계 (Descriptive Statistics)

### 📖 개념 설명

**기술 통계**는 데이터의 특성을 요약하고 설명하는 통계적 방법입니다.

| 지표 | 설명 | 계산 방법 |
|------|------|----------|
| **평균 (Mean)** | 모든 값의 합을 개수로 나눈 값 | \(\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i\) |
| **중앙값 (Median)** | 정렬 시 중앙에 위치하는 값 | 50번째 백분위수 |
| **최솟값/최댓값** | 데이터의 범위 | min(x), max(x) |
| **표준편차 (Std)** | 데이터의 퍼짐 정도 | \(\sigma = \sqrt{\frac{1}{n}\sum(x_i - \bar{x})^2}\) |

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. hours (플레이시간) 분석
   - 평균: 201.25h / 중앙값: 100.30h
   - 평균 >> 중앙값 → Right-skewed 분포 확인
   - 해석: Heavy users가 평균을 끌어올림 → Heavy/Light 구분의 근거

2. 아이템 인기도 분석
   - 평균: 1,094.3 / 중앙값: 39.0 (28배 차이!)
   - 해석: 극단적 Long-tail 분포 확인 → Popular/Long-tail 파티션 필요성 입증

3. 데이터 품질 검증
   - 결측치 비율 계산 (모두 0%)
   - 데이터 전처리 부담 최소화 확인
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 1.2 분포 분석 (Distribution Analysis)

### 📖 개념 설명

**분포 분석**은 데이터가 어떤 패턴으로 퍼져 있는지 파악하는 기법입니다.

| 분포 유형 | 특징 | 시각적 형태 |
|----------|------|-----------|
| **정규 분포** | 평균 = 중앙값, 좌우 대칭 | 종 모양 (Bell curve) |
| **Right-skewed** | 평균 > 중앙값, 오른쪽 꼬리 | 왼쪽으로 치우침 |
| **Left-skewed** | 평균 < 중앙값, 왼쪽 꼬리 | 오른쪽으로 치우침 |

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 아이템 인기도 분포 확인
   - 평균(1,094) >> 중앙값(39) = 28배 차이
   - Right-skewed 분포 → 소수의 인기 게임이 다수의 상호작용 차지
   
2. 사용자 활동량 분포 확인
   - 55%가 1개 리뷰만 작성
   - Heavy users와 Light users의 극명한 차이 확인

3. 구간별 빈도 분석 (Binning)
   - 상호작용 수 구간: 1-10, 11-100, 101-1K, ...
   - 각 구간의 비율 파악 → 파티션 기준 설정에 활용
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 1.3 유니크 값 분석 (Unique Value Count)

### 📖 개념 설명

**유니크 값 분석**은 특정 컬럼에서 중복을 제거한 고유한 값의 개수를 파악하는 기법입니다.

```python
# 예시 코드
unique_users = df['user_id'].nunique()  # 유니크 사용자 수
unique_items = df['app_id'].nunique()   # 유니크 아이템 수
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. User-Item Matrix 크기 파악
   - 유니크 사용자: 13,781,059명
   - 유니크 게임: 37,610개
   - 행렬 크기: 13.7M × 37.6K → 메모리 관리 전략 필요성 확인

2. 데이터셋 간 매칭률 확인
   - games_metadata.json app_id: 50,086개
   - games.csv app_id: 50,872개
   - 공통 app_id: 49,629개 (98.9%)
   - 병합 가능성 확인

3. Density 계산
   - 상호작용 수 / (유니크 사용자 × 유니크 아이템)
   - 41,154,794 / (13,781,059 × 37,610) = 0.0079%
   - 극도로 희소한 행렬 → Sparse Matrix 사용 근거
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 2. 분포 법칙 및 통계 개념

## 2.1 Zipf's Law (지프의 법칙)

### 📖 개념 설명

**Zipf's Law**는 자연어, 도시 인구, 웹사이트 방문 등 다양한 분야에서 관측되는 분포 법칙입니다.

```
핵심 원리:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
"빈도(frequency)는 순위(rank)에 반비례한다"

수식: f(r) ∝ 1/r^α  (α ≈ 1)

예시:
- 1위 아이템의 빈도가 F라면
- 2위 아이템의 빈도는 약 F/2
- 3위 아이템의 빈도는 약 F/3
...
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Steam 게임 인기도 분포 확인
   - 1위 (Team Fortress 2): 319,492 상호작용
   - 2위 (Rust): 270,684 (≈ 1위의 85%)
   - 10위 (Wallpaper Engine): 190,129 (≈ 1위의 60%)
   → Zipf's Law 패턴 확인

2. Long-tail 문제의 심각성 정량화
   - 상위 1% 아이템 → 전체 상호작용의 55.6%
   - 상위 10% 아이템 → 전체 상호작용의 91.6%
   → CF 모델이 상위 아이템에 편향될 수밖에 없음을 증명

3. 연구 질문(RQ1) 설정 근거
   - "Popular vs Long-tail에서 모델 성능 차이"를 분석해야 하는 이유
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 2.2 Pareto Distribution (파레토 분포)

### 📖 개념 설명

**파레토 분포**는 "80/20 법칙"으로 알려진 분포로, 소수의 요소가 전체 결과의 대부분을 차지하는 현상입니다.

```
80/20 법칙:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
"20%의 원인이 80%의 결과를 만든다"

예시:
- 20%의 고객이 80%의 매출 창출
- 20%의 버그가 80%의 시스템 오류 유발
- 20%의 게임이 80%의 플레이 시간 차지
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 누적 기여도 분석
   - 아이템별 상호작용 수 정렬 → 누적 비율 계산
   - "상위 N%가 전체의 몇 %를 차지하는가?"

2. 파티션 기준 도출 (누적 60% 기준)
   - 상위 1.23% 아이템 (463개) → 전체 상호작용의 60%
   - 이 기준으로 Popular/Long-tail 분류
   - 선행연구(ESR, 2024)에서 60%가 가장 안정적인 분할 기준임을 확인

3. 역 Pareto 분석
   - "전체의 60%를 차지하는 아이템은 몇 %인가?"
   - 답: 1.23% (463개)
   - 이 수치가 파티션 기준이 됨
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 2.3 누적 분포 함수 (CDF, Cumulative Distribution Function)

### 📖 개념 설명

**누적 분포 함수**는 특정 값 이하일 확률을 나타내는 함수입니다.

```
수식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
F(x) = P(X ≤ x)

예시 (아이템 상호작용):
- F(100) = 0.67 → 67%의 아이템이 100개 이하의 상호작용
- F(1000) = 0.90 → 90%의 아이템이 1000개 이하의 상호작용
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Pareto 누적 분포 계산
   - 아이템을 인기도 순으로 정렬
   - 각 아이템까지의 누적 상호작용 비율 계산
   - 60% 지점에서 끊어 파티션 경계 설정

2. 구체적 계산 방식:
   step 1: 인기도 순 정렬 (내림차순)
   step 2: 누적합 계산
   step 3: 전체 대비 비율 계산
   step 4: 60%에 해당하는 아이템 식별 → 18,507 상호작용 이상

3. 검증
   - Popular (462개, 0.91%) → 상호작용 60.0% ✅
   - Heavy (18.13%) → 상호작용 61.17% ✅
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 3. 추천시스템 핵심 개념

## 3.1 Implicit Feedback (암묵적 피드백)

### 📖 개념 설명

**Implicit Feedback**은 사용자가 명시적으로 평점을 주지 않았지만, 행동을 통해 간접적으로 선호를 표현하는 데이터입니다.

| 유형 | 예시 | 특징 |
|------|------|------|
| **Explicit** | 별점 1-5점, 좋아요/싫어요 | 직접적, 명확함 |
| **Implicit** | 클릭, 구매, 플레이 시간, 시청 완료 | 간접적, 대량 수집 가능 |

```
Explicit vs Implicit:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Explicit (명시적):
- Netflix: "이 영화 별 4개 줄게요"
- 장점: 선호도가 명확함
- 단점: 데이터 수집 어려움, 사용자 부담

Implicit (암묵적):
- Steam: "이 게임을 200시간 플레이했어요"
- 장점: 자연스럽게 대량 수집, 사용자 부담 없음
- 단점: 선호도 해석이 모호함 (싫어서 안 한 건지, 몰라서 안 한 건지)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Steam 데이터의 특성
   - is_recommended: 추천/비추천 (Binary Implicit)
   - hours: 플레이 시간 (Weighted Implicit)
   - 리뷰 존재 자체가 상호작용 증거

2. 두 가지 Sparse Matrix 구축 근거
   - Binary Matrix: 상호작용 존재 여부만 (1 or 0)
   - Weighted Matrix: hours를 가중치로 (선호 강도 반영)

3. 모델 선택 영향
   - Explicit 전용 모델 (SVD) 대신
   - Implicit 전용 모델 (ALS, BPR) 사용
   - 선행연구 (arXiv 2405.05562) 반영
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 3.2 Positive Bias (긍정 편향)

### 📖 개념 설명

**Positive Bias**는 사용자가 부정적인 피드백보다 긍정적인 피드백을 더 많이 남기는 현상입니다.

```
원인:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 선택 편향: 좋아하는 것만 선택 → 당연히 긍정 리뷰
2. 노력 회피: 싫으면 리뷰 안 쓰고 그냥 떠남
3. 사회적 규범: 부정적 의견 표현 꺼림

결과:
- 긍정 리뷰 >> 부정 리뷰
- 평균 평점이 실제보다 높게 형성
- 모델이 "좋다"만 학습하게 됨
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Steam 데이터에서 확인
   - is_recommended=True: 84.5%
   - is_recommended=False: 15.5%
   - 5.5:1 긍정 편향 존재

2. 모델링 시사점
   - is_recommended만으로는 선호 강도 구분 어려움
   - hours (플레이 시간)가 더 미묘한 선호 지표
   - Hours-weighted Matrix 구축의 근거

3. 평가 전략
   - 긍정 편향을 고려한 평가 필요
   - Negative sampling 전략 고려 (Phase 3에서 다룸)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 3.3 Cold-start Problem (콜드 스타트 문제)

### 📖 개념 설명

**Cold-start Problem**은 새로운 사용자나 아이템에 대해 추천을 생성하기 어려운 문제입니다.

| 유형 | 상황 | 문제점 |
|------|------|--------|
| **User Cold-start** | 신규 사용자 | 선호 정보 없음 |
| **Item Cold-start** | 신규 아이템 | 상호작용 기록 없음 |
| **System Cold-start** | 서비스 초기 | 전체 데이터 부족 |

```
CF 모델의 한계:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
협업 필터링은 "비슷한 사용자가 좋아한 것"을 추천

문제:
- 신규 사용자 → 비슷한 사용자를 찾을 수 없음
- 신규 아이템 → 아무도 상호작용하지 않았음

해결책:
1. Popularity-based: 인기 아이템 추천 (fallback)
2. Content-based: 아이템 특성 기반 추천
3. Hybrid: CF + Content 결합
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Steam 데이터에서의 Cold-start 심각성 확인
   - 55%의 사용자가 단 1개 리뷰만 작성
   - 82%의 사용자가 Light (< 4개 리뷰)
   - 67%의 게임이 Long-tail (≤100 상호작용)

2. 연구 질문(RQ2) 설정
   - "Heavy vs Light 사용자에서 모델 성능 차이"
   - Light 사용자 = Cold-start에 가까운 상황

3. 모델 설계 영향
   - Baseline: Popularity (Cold-start fallback)
   - Content-based: 태그 기반 (아이템 피처 활용)
   - Hybrid: CF + Content 결합 (Light 사용자 대응)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 3.4 Long-tail Distribution (롱테일 분포)

### 📖 개념 설명

**Long-tail Distribution**은 소수의 인기 아이템이 다수의 상호작용을 차지하고, 대다수의 아이템은 소수의 상호작용만 갖는 분포입니다.

```
시각화:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
상호작용 수
  │
  │██                                              
  │████                                            
  │██████                                          
  │████████                                        
  │██████████                                      
  │████████████                                    
  │██████████████                                  
  │████████████████████████████████████████████████
  └──────────────────────────────────────────────→ 아이템 순위
    ↑ Head (Popular)        ↑ Long-tail
    (소수, 고인기)           (다수, 저인기)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Steam 데이터의 극단적 Long-tail 확인
   - Popular (0.91%, 462개): 상호작용 60%
   - Long-tail (99.09%, 50,410개): 상호작용 40%
   - 인기도 평균(1,094) vs 중앙값(39) = 28배 차이

2. 연구 질문(RQ1) 핵심
   - "Popular vs Long-tail에서 CF 성능 차이"
   - Long-tail에서 CF 성능 급락 예상
   - Content-based로 보완 필요

3. 평가 지표 설계
   - Coverage@K: Long-tail 아이템을 얼마나 추천하는가?
   - Novelty@K: 추천된 아이템의 비인기도
   - 선행연구 (Salkanović, 2024) 반영
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 3.5 Popularity Bias (인기 편향)

### 📖 개념 설명

**Popularity Bias**는 추천 시스템이 인기 아이템을 과도하게 추천하는 경향입니다.

```
원인:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 데이터 편향: 인기 아이템의 상호작용 데이터가 풍부
2. 모델 학습: 데이터가 많은 아이템을 더 잘 학습
3. 피드백 루프: 추천 → 상호작용 증가 → 더 추천

결과:
- "부자가 더 부자가 된다" (Rich get richer)
- Long-tail 아이템 노출 기회 감소
- 사용자 다양성 경험 저하
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 문제 정의
   - CF 모델이 인기 게임만 추천할 것으로 예상
   - Long-tail 게임은 추천 기회 박탈
   - 다양성 부족 문제 발생

2. 선행연구 기반 해결 방향 (Wang, 2025)
   - Hybrid 모델에서 α 조절
   - Popular 아이템: CF 비중 ↑
   - Long-tail 아이템: Content 비중 ↑

3. 평가 지표로 측정
   - Coverage@K: 추천된 유니크 아이템 비율
   - Novelty@K: 추천 아이템의 평균 비인기도
   - 인기 편향이 심할수록 낮은 값
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 4. 데이터 분할 기법

## 4.1 Train/Valid/Test Split

### 📖 개념 설명

**Train/Valid/Test Split**은 데이터를 학습, 검증, 테스트 세 부분으로 나누는 기법입니다.

| 데이터셋 | 용도 | 비율 (일반적) |
|----------|------|-------------|
| **Train** | 모델 학습 | 70-80% |
| **Validation** | 하이퍼파라미터 튜닝 | 10-15% |
| **Test** | 최종 성능 평가 | 10-15% |

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. User-based Split (80/10/10)
   - Train: 32,922,939 rows (80%)
   - Valid: 4,110,632 rows (10%)
   - Test: 4,121,223 rows (10%)

2. 분할 방식: 사용자 단위
   - 전체 사용자를 80/10/10으로 분할
   - 같은 사용자의 상호작용은 같은 셋에 배치
   - Cold User 시나리오 시뮬레이션

3. 용도
   - Popularity 모델: Cold User에서도 작동
   - Content-based: Cold User 대응
   - 신규 사용자 추천 성능 평가
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 4.2 Leave-One-Out (LOO)

### 📖 개념 설명

**Leave-One-Out**은 각 사용자의 마지막 상호작용을 테스트로 분리하는 기법입니다.

```
방식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
사용자 A의 상호작용: [게임1, 게임2, 게임3, 게임4, 게임5]
                      ↓
Train: [게임1, 게임2, 게임3, 게임4]
Test: [게임5]

장점:
- 각 사용자가 Train과 Test에 모두 존재
- CF 모델의 "개인화" 성능 평가 가능
- 시간 순서 반영 (마지막 상호작용 예측)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. CF 모델 평가용 분할
   - User-based Split은 CF 평가에 부적합 (Cold User만 존재)
   - LOO는 모든 사용자가 Train에 존재 → CF 학습 가능

2. LOO 분할 결과
   - LOO Train: 21,898,092 rows
   - LOO Test: 4,964,722 rows
   - 사용자 수: 4,965,184명 (Train에 2개 이상 상호작용 필요)

3. 분할 과정
   step 1: 날짜순 정렬 (date 컬럼 활용)
   step 2: 각 사용자의 마지막 상호작용 → Test
   step 3: 나머지 → Train
   step 4: Train에 1개 이상 상호작용 있는 사용자만 유지
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 5. 데이터 구조 및 변환

## 5.1 Sparse Matrix (희소 행렬)

### 📖 개념 설명

**Sparse Matrix**는 대부분의 요소가 0인 행렬을 효율적으로 저장하는 자료구조입니다.

```
일반 행렬 vs 희소 행렬:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
일반 행렬 (Dense Matrix):
┌───┬───┬───┬───┬───┐
│ 0 │ 0 │ 1 │ 0 │ 0 │
│ 0 │ 0 │ 0 │ 0 │ 1 │
│ 1 │ 0 │ 0 │ 0 │ 0 │
└───┴───┴───┴───┴───┘
메모리: 5 × 3 × 8bytes = 120 bytes

희소 행렬 (CSR Format):
data:    [1, 1, 1]      (값만 저장)
indices: [2, 4, 0]      (열 인덱스)
indptr:  [0, 1, 2, 3]   (행 시작점)
메모리: (3 + 3 + 4) × 4bytes = 40 bytes
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 메모리 효율성 (필수!)
   - User-Item Matrix 크기: 4,965,184 × 37,251
   - Dense 저장 시: 185B cells × 8bytes ≈ 1.48 TB (불가능!)
   - Sparse 저장 시: 21.9M non-zero × 12bytes ≈ 263 MB

2. CSR (Compressed Sparse Row) 포맷 사용
   - 행 단위 접근에 최적화
   - 사용자별 상호작용 조회에 효율적
   - scipy.sparse.csr_matrix 사용

3. 두 가지 버전 구축
   - Binary Matrix: 상호작용 존재 = 1
   - Weighted Matrix: 상호작용 강도 = log1p(hours)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 5.2 ID Mapping (ID 매핑)

### 📖 개념 설명

**ID Mapping**은 원본 ID를 연속적인 정수 인덱스로 변환하는 과정입니다.

```
왜 필요한가:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
원본 user_id: [1001234567, 9876543210, 5555555555, ...]
              ↓ 변환 ↓
user_idx:     [0, 1, 2, ...]

이유:
1. 행렬 인덱싱은 0부터 시작하는 연속 정수 필요
2. 원본 ID가 불연속적이면 메모리 낭비
3. 빠른 조회를 위한 해시 테이블 구축
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 매핑 딕셔너리 생성
   user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}
   item_to_idx = {app_id: idx for idx, app_id in enumerate(unique_items)}

2. 역매핑 (추천 결과 해석용)
   idx_to_user = {v: k for k, v in user_to_idx.items()}
   idx_to_item = {v: k for k, v in item_to_idx.items()}

3. Pickle로 저장
   - id_mappings.pkl: 전체 데이터용
   - loo_mappings.pkl: LOO 분할용
   - 모델 학습/추론 시 동일 매핑 보장
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 5.3 Chunk-based Processing (청크 기반 처리)

### 📖 개념 설명

**Chunk-based Processing**은 대용량 데이터를 작은 단위로 나누어 순차적으로 처리하는 기법입니다.

```
방식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
전체 데이터: 41M rows (1.88 GB)
              ↓ 분할 ↓
Chunk 1: 1M rows → 처리 → 결과 저장
Chunk 2: 1M rows → 처리 → 결과 병합
...
Chunk 42: 0.15M rows → 처리 → 최종 결과

장점:
- 메모리 부족 방지
- 진행 상황 추적 가능
- 중간 실패 시 재개 가능
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. recommendations.csv 처리
   - 파일 크기: 1.88 GB, 41M rows
   - 한 번에 로드 시 메모리 부족 위험
   - 1M rows씩 청크 처리

2. 구현 방식:
   for chunk in pd.read_csv(file, chunksize=1_000_000):
       # 청크별 집계
       chunk_counts = chunk.groupby('app_id').size()
       # 결과 누적
       total_counts = total_counts.add(chunk_counts, fill_value=0)

3. 사용된 Task
   - Task 1.5: Zipf/Pareto 분석
   - Task 1.6: 상호작용 집계
   - Task 2.1: 파티션 교차 분석
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 6. 평가 지표

## 6.1 Recall@K

### 📖 개념 설명

**Recall@K**는 실제 관련 아이템 중 상위 K개 추천에 포함된 비율입니다.

```
수식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Recall@K = |추천된 K개 ∩ 실제 관련 아이템| / |실제 관련 아이템|

예시:
- 실제 선호 아이템: [A, B, C] (3개)
- 추천 Top-5: [A, D, B, E, F]
- 적중: [A, B] (2개)
- Recall@5 = 2/3 = 0.67
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 추천 정확도 측정
   - "사용자가 좋아할 아이템을 얼마나 찾았는가?"
   - 높을수록 좋음 (0~1)

2. Implicit Feedback에 적합
   - Binary 피드백 (좋아요/안좋아요)에서 자주 사용
   - LOO Test에서 1개 아이템을 맞추면 Recall=1

3. K 값 선택
   - K=5, 10, 20
   - 실제 서비스에서 보여주는 추천 수와 유사
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 6.2 NDCG@K (Normalized Discounted Cumulative Gain)

### 📖 개념 설명

**NDCG@K**는 순위를 고려한 추천 품질 지표입니다. 상위에 관련 아이템이 있을수록 높은 점수를 받습니다.

```
수식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
DCG@K = Σ (rel_i / log2(i+1))  for i = 1 to K
IDCG@K = 이상적인 순서일 때의 DCG
NDCG@K = DCG@K / IDCG@K

예시:
- 추천: [A(관련), D(무관), B(관련), E(무관), F(무관)]
- DCG@5 = 1/log2(2) + 0/log2(3) + 1/log2(4) + 0/log2(5) + 0/log2(6)
        = 1.0 + 0.5 = 1.5
- IDCG@5 = 1/log2(2) + 1/log2(3) = 1.63 (2개 관련 아이템)
- NDCG@5 = 1.5/1.63 = 0.92
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 순위 품질 평가
   - Recall은 순위를 무시 (1위든 10위든 동일)
   - NDCG는 상위 순위에 더 높은 가중치
   - "좋은 아이템이 위에 있는가?"

2. 사용자 경험 반영
   - 사용자는 상위 추천만 보는 경향
   - 1위에 관련 아이템 > 10위에 관련 아이템

3. 선행연구에서 표준 지표
   - RecSys, NeurIPS 논문에서 필수 지표
   - 비교 가능성 확보
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 6.3 Coverage@K

### 📖 개념 설명

**Coverage@K**는 전체 아이템 중 추천에 등장한 유니크 아이템의 비율입니다.

```
수식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Coverage@K = |모든 사용자의 Top-K 추천에 등장한 유니크 아이템| / |전체 아이템|

예시:
- 전체 아이템: 37,610개
- 모든 사용자의 Top-10 추천 통합: 500개 유니크 아이템
- Coverage@10 = 500/37,610 = 1.33%
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Long-tail 노출 측정
   - 인기 편향이 심하면 Coverage 낮음
   - Long-tail 아이템이 추천되면 Coverage 높음

2. 선행연구(Salkanović, 2024) 반영
   - Steam 다양성 연구에서 핵심 지표로 사용
   - "얼마나 다양한 게임을 추천하는가?"

3. 해석
   - Coverage가 낮음 → 같은 인기 아이템만 추천
   - Coverage가 높음 → 다양한 아이템 추천 (Long-tail 포함)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 6.4 Novelty@K

### 📖 개념 설명

**Novelty@K**는 추천된 아이템의 평균 비인기도입니다. 높을수록 Long-tail 아이템을 많이 추천합니다.

```
수식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Novelty@K = 평균(1 - 정규화된 인기도)

인기도 = 해당 아이템의 상호작용 수
정규화 = 인기도 / 최대 인기도

예시:
- 최대 인기도: 319,492 (Team Fortress 2)
- 추천 아이템 인기도: [319,492, 100,000, 50]
- Novelty = (1-1.0 + 1-0.31 + 1-0.0002) / 3 = 0.56
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 인기 편향 정량화
   - Novelty 낮음 → 인기 아이템 위주 추천
   - Novelty 높음 → Long-tail 아이템 추천

2. Coverage와 상호 보완
   - Coverage: 추천된 아이템의 "개수"
   - Novelty: 추천된 아이템의 "특성" (비인기도)

3. RQ1 분석에 활용
   - CF vs Content-based의 Novelty 비교
   - Hybrid가 Long-tail 추천을 늘리는지 확인
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 7. 피처 엔지니어링

## 7.1 Log Transformation (로그 변환)

### 📖 개념 설명

**Log Transformation**은 Right-skewed 분포를 정규 분포에 가깝게 변환하는 기법입니다.

```
수식:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
x' = log(x + 1)  또는  log1p(x)

왜 +1을 더하는가?
- log(0) = -∞ (정의되지 않음)
- log(1) = 0
- x=0일 때도 안전하게 처리

효과:
- 큰 값의 영향력 감소
- 분포가 정규 분포에 가까워짐
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. hours를 가중치로 사용 시
   - 원본 hours 분포: 0 ~ 999.90 (극단적 차이)
   - 1시간 vs 100시간 → 100배 차이
   - log1p 적용 후: 0 ~ 6.91 (완만한 차이)

2. Weighted Sparse Matrix 구축
   weighted_hours = np.log1p(hours)
   - 극단적 플레이어의 영향력 완화
   - 1시간 플레이도 의미 있게 반영

3. 선행연구 기반
   - Hu et al. (2008): ALS 논문에서 log 변환 권장
   - "confidence = 1 + α·log(1 + count)"
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 7.2 Multi-hot Encoding (멀티-핫 인코딩)

### 📖 개념 설명

**Multi-hot Encoding**은 한 샘플이 여러 카테고리에 속할 때 사용하는 벡터화 기법입니다.

```
One-hot vs Multi-hot:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
One-hot (단일 카테고리):
- 장르: RPG → [0, 0, 0, 1, 0, 0]  (1개만 1)

Multi-hot (다중 카테고리):
- 태그: [Action, RPG, Indie] → [1, 0, 0, 1, 1, 0]  (여러 개 1)

예시 (게임 태그):
전체 태그: [Action, Adventure, RPG, Indie, Strategy, ...]
게임 A 태그: [Action, RPG] → [1, 0, 1, 0, 0, ...]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 태그 벡터화 (Phase 4에서 사용 예정)
   - 431개 유니크 태그
   - 각 게임: 평균 12.8개 태그
   - Multi-hot 벡터: [0, 1, 0, 1, 1, 0, 0, 1, ...]

2. Content-based 모델의 핵심 피처
   - 게임 간 유사도 계산: cosine_similarity(tag_vector_A, tag_vector_B)
   - 태그가 비슷할수록 높은 유사도

3. 장르(genres)에도 동일 적용
   - genres: ['Action', 'Free To Play']
   - Multi-hot 변환 후 피처로 활용
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 7.3 Data Type Optimization (데이터 타입 최적화)

### 📖 개념 설명

**데이터 타입 최적화**는 메모리 사용량을 줄이기 위해 적절한 데이터 타입을 선택하는 기법입니다.

```
숫자형 데이터 타입:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
| 타입     | 바이트 | 범위                          |
|----------|--------|-------------------------------|
| int8     | 1      | -128 ~ 127                    |
| int16    | 2      | -32,768 ~ 32,767              |
| int32    | 4      | -2.1B ~ 2.1B                  |
| int64    | 8      | -9.2E ~ 9.2E                  |
| float32  | 4      | ±3.4E38                       |
| float64  | 8      | ±1.8E308                      |
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

### 🎯 왜 사용했는가?

```
본 프로젝트에서의 활용:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. 메모리 절약
   - user_id: int64 → int32 (범위 충분)
   - hours: float64 → float32 (정밀도 충분)
   - 50% 메모리 절약 가능

2. 대용량 데이터 처리 시 필수
   - 41M rows × 8 columns
   - int64 기본: 41M × 8 × 8bytes = 2.6 GB
   - int32 변환: 41M × 8 × 4bytes = 1.3 GB

3. Sparse Matrix에서도 적용
   - data 배열: float64 → float32
   - indices 배열: int64 → int32
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

# 📊 요약표

| 개념 | 카테고리 | 사용 이유 |
|------|----------|----------|
| 기술 통계 | EDA | 데이터 분포 파악, Heavy/Light 구분 근거 |
| 분포 분석 | EDA | Right-skewed 확인, 파티션 필요성 입증 |
| Zipf's Law | 분포 법칙 | Long-tail 심각성 정량화 |
| Pareto 분포 | 분포 법칙 | 누적 60% 기준 파티션 도출 |
| Implicit Feedback | 추천시스템 | hours 기반 선호 강도 모델링 |
| Positive Bias | 추천시스템 | hours가 is_recommended보다 유용한 이유 |
| Cold-start | 추천시스템 | Light 사용자 대응 전략 설계 |
| Long-tail | 추천시스템 | RQ1 설정, Content 필요성 |
| Popularity Bias | 추천시스템 | Coverage, Novelty 지표 도입 |
| Train/Valid/Test | 데이터 분할 | Cold User 평가 시나리오 |
| Leave-One-Out | 데이터 분할 | CF 모델 평가 시나리오 |
| Sparse Matrix | 데이터 구조 | 메모리 효율적 저장 (2.6GB → 263MB) |
| ID Mapping | 데이터 변환 | 행렬 인덱싱 가능하게 |
| Chunk Processing | 처리 기법 | 대용량 파일 메모리 부족 방지 |
| Recall@K | 평가 지표 | 추천 정확도 측정 |
| NDCG@K | 평가 지표 | 순위 품질 측정 |
| Coverage@K | 평가 지표 | Long-tail 노출 측정 |
| Novelty@K | 평가 지표 | 인기 편향 정량화 |
| Log Transform | 피처 엔지니어링 | hours 가중치 정규화 |
| Multi-hot | 피처 엔지니어링 | 태그 벡터화 |
| 타입 최적화 | 피처 엔지니어링 | 메모리 50% 절약 |

---

*작성일: 2024.12.05*  
*경희대학교 빅데이터응용학과 추천시스템 프로젝트*

